#!/usr/bin/env python3
"""
Backup and Recovery System for Pharmaceutical Manufacturing
FDA 21 CFR Part 11 Compliant Data Protection and Disaster Recovery
"""

import os
import sys
import asyncio
import logging
import shutil
import json
import gzip
import hashlib
from datetime import datetime, timezone, timedelta
from pathlib import Path
from typing import Dict, List, Any, Optional
import subprocess

import asyncpg
from cryptography.fernet import Fernet

# Add the src directory to the path
sys.path.append(str(Path(__file__).parent.parent / 'src'))

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class PharmaBackupRecovery:
    """Backup and recovery system for pharmaceutical manufacturing data"""
    
    def __init__(self):
        # Database configuration
        self.db_host = os.getenv('DB_HOST', 'postgres')
        self.db_port = int(os.getenv('DB_PORT', '5432'))
        self.db_user = os.getenv('DB_USER', 'pharma_user')
        self.db_password = os.getenv('DB_PASSWORD', 'pharma_password')
        self.db_name = os.getenv('DB_NAME', 'pharma_manufacturing_db')
        
        # Backup configuration
        self.backup_dir = Path(os.getenv('BACKUP_DIRECTORY', '/app/backups'))
        self.data_dir = Path(os.getenv('DATA_DIRECTORY', '/app/data'))
        self.retention_days = int(os.getenv('BACKUP_RETENTION_DAYS', '90'))
        self.encryption_enabled = os.getenv('BACKUP_ENCRYPTION_ENABLED', 'true').lower() == 'true'
        self.compression_enabled = os.getenv('BACKUP_COMPRESSION_ENABLED', 'true').lower() == 'true'
        
        # Regulatory compliance
        self.cfr_compliance = os.getenv('CFR_21_PART_11_COMPLIANCE', 'true').lower() == 'true'
        self.audit_backup_operations = True\n        \n        # Create directories\n        self.backup_dir.mkdir(parents=True, exist_ok=True)\n        self.data_dir.mkdir(parents=True, exist_ok=True)\n        \n        # Encryption key (in production, this should be from a secure key management service)\n        self.encryption_key = self.get_or_create_encryption_key()\n        \n        # Backup metadata\n        self.backup_metadata = {\n            'system': 'pharmaceutical_manufacturing',\n            'compliance_level': 'FDA_21_CFR_Part_11',\n            'encryption_enabled': self.encryption_enabled,\n            'compression_enabled': self.compression_enabled\n        }\n    \n    def get_or_create_encryption_key(self) -> bytes:\n        \"\"\"Get or create encryption key for backup data\"\"\"\n        key_file = self.backup_dir / '.backup_key'\n        \n        if key_file.exists():\n            with open(key_file, 'rb') as f:\n                return f.read()\n        else:\n            key = Fernet.generate_key()\n            with open(key_file, 'wb') as f:\n                f.write(key)\n            # Secure the key file\n            os.chmod(key_file, 0o600)\n            return key\n    \n    def calculate_checksum(self, file_path: Path) -> str:\n        \"\"\"Calculate SHA-256 checksum for data integrity verification\"\"\"\n        sha256_hash = hashlib.sha256()\n        with open(file_path, \"rb\") as f:\n            for byte_block in iter(lambda: f.read(4096), b\"\"):\n                sha256_hash.update(byte_block)\n        return sha256_hash.hexdigest()\n    \n    def encrypt_file(self, file_path: Path, output_path: Path) -> bool:\n        \"\"\"Encrypt file for secure storage\"\"\"\n        try:\n            fernet = Fernet(self.encryption_key)\n            \n            with open(file_path, 'rb') as input_file:\n                file_data = input_file.read()\n            \n            encrypted_data = fernet.encrypt(file_data)\n            \n            with open(output_path, 'wb') as output_file:\n                output_file.write(encrypted_data)\n            \n            return True\n        except Exception as e:\n            logger.error(f\"File encryption failed: {str(e)}\")\n            return False\n    \n    def decrypt_file(self, encrypted_path: Path, output_path: Path) -> bool:\n        \"\"\"Decrypt file for restoration\"\"\"\n        try:\n            fernet = Fernet(self.encryption_key)\n            \n            with open(encrypted_path, 'rb') as encrypted_file:\n                encrypted_data = encrypted_file.read()\n            \n            decrypted_data = fernet.decrypt(encrypted_data)\n            \n            with open(output_path, 'wb') as output_file:\n                output_file.write(decrypted_data)\n            \n            return True\n        except Exception as e:\n            logger.error(f\"File decryption failed: {str(e)}\")\n            return False\n    \n    def compress_file(self, file_path: Path, output_path: Path) -> bool:\n        \"\"\"Compress file to reduce storage space\"\"\"\n        try:\n            with open(file_path, 'rb') as input_file:\n                with gzip.open(output_path, 'wb') as output_file:\n                    shutil.copyfileobj(input_file, output_file)\n            return True\n        except Exception as e:\n            logger.error(f\"File compression failed: {str(e)}\")\n            return False\n    \n    def decompress_file(self, compressed_path: Path, output_path: Path) -> bool:\n        \"\"\"Decompress file for restoration\"\"\"\n        try:\n            with gzip.open(compressed_path, 'rb') as input_file:\n                with open(output_path, 'wb') as output_file:\n                    shutil.copyfileobj(input_file, output_file)\n            return True\n        except Exception as e:\n            logger.error(f\"File decompression failed: {str(e)}\")\n            return False\n    \n    async def backup_database(self, backup_name: str) -> Dict[str, Any]:\n        \"\"\"Create database backup with regulatory compliance\"\"\"\n        try:\n            logger.info(f\"Starting database backup: {backup_name}\")\n            \n            timestamp = datetime.now(timezone.utc)\n            backup_file = self.backup_dir / f\"db_{backup_name}_{timestamp.strftime('%Y%m%d_%H%M%S')}.sql\"\n            \n            # Create database dump using pg_dump\n            pg_dump_cmd = [\n                'pg_dump',\n                f'--host={self.db_host}',\n                f'--port={self.db_port}',\n                f'--username={self.db_user}',\n                f'--dbname={self.db_name}',\n                '--verbose',\n                '--clean',\n                '--if-exists',\n                '--create',\n                '--format=plain',\n                f'--file={backup_file}'\n            ]\n            \n            # Set password environment variable\n            env = os.environ.copy()\n            env['PGPASSWORD'] = self.db_password\n            \n            # Execute pg_dump\n            result = subprocess.run(\n                pg_dump_cmd,\n                env=env,\n                capture_output=True,\n                text=True,\n                timeout=3600  # 1 hour timeout\n            )\n            \n            if result.returncode != 0:\n                raise Exception(f\"pg_dump failed: {result.stderr}\")\n            \n            # Verify backup file was created\n            if not backup_file.exists():\n                raise Exception(\"Backup file was not created\")\n            \n            backup_size = backup_file.stat().st_size\n            logger.info(f\"Database backup created: {backup_file} ({backup_size} bytes)\")\n            \n            # Calculate checksum for integrity\n            checksum = self.calculate_checksum(backup_file)\n            \n            # Process backup file (compression and encryption)\n            final_backup_file = backup_file\n            \n            if self.compression_enabled:\n                compressed_file = backup_file.with_suffix('.sql.gz')\n                if self.compress_file(backup_file, compressed_file):\n                    backup_file.unlink()  # Remove uncompressed file\n                    final_backup_file = compressed_file\n                    logger.info(f\"Backup compressed: {compressed_file}\")\n            \n            if self.encryption_enabled:\n                encrypted_file = final_backup_file.with_suffix(final_backup_file.suffix + '.enc')\n                if self.encrypt_file(final_backup_file, encrypted_file):\n                    final_backup_file.unlink()  # Remove unencrypted file\n                    final_backup_file = encrypted_file\n                    logger.info(f\"Backup encrypted: {encrypted_file}\")\n            \n            # Create backup metadata\n            metadata = {\n                **self.backup_metadata,\n                'backup_type': 'database',\n                'backup_name': backup_name,\n                'timestamp': timestamp.isoformat(),\n                'original_file': str(backup_file),\n                'final_file': str(final_backup_file),\n                'original_size': backup_size,\n                'final_size': final_backup_file.stat().st_size,\n                'checksum': checksum,\n                'compression_ratio': backup_size / final_backup_file.stat().st_size if self.compression_enabled else 1.0,\n                'database_info': {\n                    'host': self.db_host,\n                    'database': self.db_name,\n                    'user': self.db_user\n                }\n            }\n            \n            # Save metadata\n            metadata_file = final_backup_file.with_suffix('.metadata.json')\n            with open(metadata_file, 'w') as f:\n                json.dump(metadata, f, indent=2)\n            \n            logger.info(f\"Database backup completed successfully: {final_backup_file}\")\n            return metadata\n            \n        except Exception as e:\n            logger.error(f\"Database backup failed: {str(e)}\")\n            raise\n    \n    async def backup_application_data(self, backup_name: str) -> Dict[str, Any]:\n        \"\"\"Backup application data files with regulatory compliance\"\"\"\n        try:\n            logger.info(f\"Starting application data backup: {backup_name}\")\n            \n            timestamp = datetime.now(timezone.utc)\n            backup_archive = self.backup_dir / f\"data_{backup_name}_{timestamp.strftime('%Y%m%d_%H%M%S')}.tar\"\n            \n            # Create tar archive of data directory\n            tar_cmd = [\n                'tar',\n                '-cf',\n                str(backup_archive),\n                '-C',\n                str(self.data_dir.parent),\n                self.data_dir.name\n            ]\n            \n            result = subprocess.run(\n                tar_cmd,\n                capture_output=True,\n                text=True,\n                timeout=1800  # 30 minutes timeout\n            )\n            \n            if result.returncode != 0:\n                raise Exception(f\"Data backup failed: {result.stderr}\")\n            \n            backup_size = backup_archive.stat().st_size\n            logger.info(f\"Data backup created: {backup_archive} ({backup_size} bytes)\")\n            \n            # Calculate checksum\n            checksum = self.calculate_checksum(backup_archive)\n            \n            # Process backup file\n            final_backup_file = backup_archive\n            \n            if self.compression_enabled:\n                compressed_file = backup_archive.with_suffix('.tar.gz')\n                if self.compress_file(backup_archive, compressed_file):\n                    backup_archive.unlink()\n                    final_backup_file = compressed_file\n                    logger.info(f\"Data backup compressed: {compressed_file}\")\n            \n            if self.encryption_enabled:\n                encrypted_file = final_backup_file.with_suffix(final_backup_file.suffix + '.enc')\n                if self.encrypt_file(final_backup_file, encrypted_file):\n                    final_backup_file.unlink()\n                    final_backup_file = encrypted_file\n                    logger.info(f\"Data backup encrypted: {encrypted_file}\")\n            \n            # Create metadata\n            metadata = {\n                **self.backup_metadata,\n                'backup_type': 'application_data',\n                'backup_name': backup_name,\n                'timestamp': timestamp.isoformat(),\n                'original_file': str(backup_archive),\n                'final_file': str(final_backup_file),\n                'original_size': backup_size,\n                'final_size': final_backup_file.stat().st_size,\n                'checksum': checksum,\n                'data_directory': str(self.data_dir)\n            }\n            \n            metadata_file = final_backup_file.with_suffix('.metadata.json')\n            with open(metadata_file, 'w') as f:\n                json.dump(metadata, f, indent=2)\n            \n            logger.info(f\"Application data backup completed: {final_backup_file}\")\n            return metadata\n            \n        except Exception as e:\n            logger.error(f\"Application data backup failed: {str(e)}\")\n            raise\n    \n    async def create_full_backup(self, backup_name: Optional[str] = None) -> Dict[str, Any]:\n        \"\"\"Create complete system backup\"\"\"\n        try:\n            if not backup_name:\n                backup_name = f\"full_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n            \n            logger.info(f\"Starting full system backup: {backup_name}\")\n            \n            # Create backups concurrently\n            db_backup, data_backup = await asyncio.gather(\n                self.backup_database(f\"{backup_name}_db\"),\n                self.backup_application_data(f\"{backup_name}_data\")\n            )\n            \n            # Create full backup metadata\n            full_backup_metadata = {\n                'backup_type': 'full_system',\n                'backup_name': backup_name,\n                'timestamp': datetime.now(timezone.utc).isoformat(),\n                'components': {\n                    'database': db_backup,\n                    'application_data': data_backup\n                },\n                'regulatory_compliance': {\n                    'cfr_21_part_11': self.cfr_compliance,\n                    'data_integrity_verified': True,\n                    'audit_trail_included': True,\n                    'retention_period_days': self.retention_days\n                }\n            }\n            \n            # Save full backup metadata\n            full_metadata_file = self.backup_dir / f\"full_backup_{backup_name}.metadata.json\"\n            with open(full_metadata_file, 'w') as f:\n                json.dump(full_backup_metadata, f, indent=2)\n            \n            logger.info(f\"Full system backup completed: {backup_name}\")\n            return full_backup_metadata\n            \n        except Exception as e:\n            logger.error(f\"Full backup failed: {str(e)}\")\n            raise\n    \n    async def restore_database(self, backup_file: Path) -> bool:\n        \"\"\"Restore database from backup\"\"\"\n        try:\n            logger.info(f\"Starting database restoration from: {backup_file}\")\n            \n            # Process backup file (decrypt and decompress if needed)\n            restore_file = backup_file\n            \n            if backup_file.suffix == '.enc':\n                decrypted_file = backup_file.with_suffix('')\n                if not self.decrypt_file(backup_file, decrypted_file):\n                    raise Exception(\"Failed to decrypt backup file\")\n                restore_file = decrypted_file\n            \n            if restore_file.suffix == '.gz':\n                decompressed_file = restore_file.with_suffix('')\n                if not self.decompress_file(restore_file, decompressed_file):\n                    raise Exception(\"Failed to decompress backup file\")\n                if restore_file != backup_file:\n                    restore_file.unlink()  # Remove temporary decrypted file\n                restore_file = decompressed_file\n            \n            # Restore database using psql\n            psql_cmd = [\n                'psql',\n                f'--host={self.db_host}',\n                f'--port={self.db_port}',\n                f'--username={self.db_user}',\n                f'--dbname=postgres',  # Connect to postgres database first\n                '--file=' + str(restore_file)\n            ]\n            \n            env = os.environ.copy()\n            env['PGPASSWORD'] = self.db_password\n            \n            result = subprocess.run(\n                psql_cmd,\n                env=env,\n                capture_output=True,\n                text=True,\n                timeout=3600\n            )\n            \n            # Clean up temporary files\n            if restore_file != backup_file:\n                restore_file.unlink()\n            \n            if result.returncode != 0:\n                raise Exception(f\"Database restoration failed: {result.stderr}\")\n            \n            logger.info(\"Database restoration completed successfully\")\n            return True\n            \n        except Exception as e:\n            logger.error(f\"Database restoration failed: {str(e)}\")\n            return False\n    \n    async def restore_application_data(self, backup_file: Path) -> bool:\n        \"\"\"Restore application data from backup\"\"\"\n        try:\n            logger.info(f\"Starting data restoration from: {backup_file}\")\n            \n            # Process backup file\n            restore_file = backup_file\n            \n            if backup_file.suffix == '.enc':\n                decrypted_file = backup_file.with_suffix('')\n                if not self.decrypt_file(backup_file, decrypted_file):\n                    raise Exception(\"Failed to decrypt backup file\")\n                restore_file = decrypted_file\n            \n            if restore_file.suffix == '.gz':\n                decompressed_file = restore_file.with_suffix('')\n                if not self.decompress_file(restore_file, decompressed_file):\n                    raise Exception(\"Failed to decompress backup file\")\n                if restore_file != backup_file:\n                    restore_file.unlink()\n                restore_file = decompressed_file\n            \n            # Backup current data directory\n            backup_current = self.data_dir.with_suffix('.backup_' + datetime.now().strftime('%Y%m%d_%H%M%S'))\n            if self.data_dir.exists():\n                shutil.move(str(self.data_dir), str(backup_current))\n            \n            # Extract backup\n            tar_cmd = [\n                'tar',\n                '-xf',\n                str(restore_file),\n                '-C',\n                str(self.data_dir.parent)\n            ]\n            \n            result = subprocess.run(\n                tar_cmd,\n                capture_output=True,\n                text=True,\n                timeout=1800\n            )\n            \n            # Clean up temporary files\n            if restore_file != backup_file:\n                restore_file.unlink()\n            \n            if result.returncode != 0:\n                # Restore original data on failure\n                if backup_current.exists():\n                    shutil.move(str(backup_current), str(self.data_dir))\n                raise Exception(f\"Data restoration failed: {result.stderr}\")\n            \n            # Remove backup of current data on success\n            if backup_current.exists():\n                shutil.rmtree(backup_current)\n            \n            logger.info(\"Application data restoration completed successfully\")\n            return True\n            \n        except Exception as e:\n            logger.error(f\"Application data restoration failed: {str(e)}\")\n            return False\n    \n    async def list_backups(self) -> List[Dict[str, Any]]:\n        \"\"\"List available backups with metadata\"\"\"\n        backups = []\n        \n        for metadata_file in self.backup_dir.glob('*.metadata.json'):\n            try:\n                with open(metadata_file, 'r') as f:\n                    metadata = json.load(f)\n                \n                # Check if backup file still exists\n                backup_file = Path(metadata.get('final_file', ''))\n                if backup_file.exists():\n                    metadata['file_exists'] = True\n                    metadata['current_size'] = backup_file.stat().st_size\n                else:\n                    metadata['file_exists'] = False\n                \n                backups.append(metadata)\n                \n            except Exception as e:\n                logger.warning(f\"Failed to read backup metadata {metadata_file}: {str(e)}\")\n        \n        # Sort by timestamp (newest first)\n        backups.sort(key=lambda x: x.get('timestamp', ''), reverse=True)\n        return backups\n    \n    async def cleanup_old_backups(self) -> int:\n        \"\"\"Remove backups older than retention period\"\"\"\n        try:\n            cutoff_date = datetime.now(timezone.utc) - timedelta(days=self.retention_days)\n            removed_count = 0\n            \n            backups = await self.list_backups()\n            \n            for backup in backups:\n                backup_date = datetime.fromisoformat(backup['timestamp'].replace('Z', '+00:00'))\n                \n                if backup_date < cutoff_date:\n                    # Remove backup file\n                    backup_file = Path(backup.get('final_file', ''))\n                    if backup_file.exists():\n                        backup_file.unlink()\n                        logger.info(f\"Removed old backup: {backup_file}\")\n                    \n                    # Remove metadata file\n                    metadata_file = backup_file.with_suffix('.metadata.json')\n                    if metadata_file.exists():\n                        metadata_file.unlink()\n                    \n                    removed_count += 1\n            \n            logger.info(f\"Cleanup completed: {removed_count} old backups removed\")\n            return removed_count\n            \n        except Exception as e:\n            logger.error(f\"Backup cleanup failed: {str(e)}\")\n            return 0\n    \n    async def verify_backup_integrity(self, backup_file: Path) -> bool:\n        \"\"\"Verify backup file integrity using checksum\"\"\"\n        try:\n            metadata_file = backup_file.with_suffix('.metadata.json')\n            \n            if not metadata_file.exists():\n                logger.error(f\"Metadata file not found: {metadata_file}\")\n                return False\n            \n            with open(metadata_file, 'r') as f:\n                metadata = json.load(f)\n            \n            expected_checksum = metadata.get('checksum')\n            if not expected_checksum:\n                logger.error(\"No checksum found in metadata\")\n                return False\n            \n            # Calculate current checksum\n            current_checksum = self.calculate_checksum(backup_file)\n            \n            if current_checksum == expected_checksum:\n                logger.info(f\"Backup integrity verified: {backup_file}\")\n                return True\n            else:\n                logger.error(f\"Backup integrity check failed: {backup_file}\")\n                return False\n                \n        except Exception as e:\n            logger.error(f\"Backup integrity verification failed: {str(e)}\")\n            return False\n\nasync def main():\n    \"\"\"Main backup/recovery function\"\"\"\n    import argparse\n    \n    parser = argparse.ArgumentParser(description='Pharmaceutical Manufacturing Backup/Recovery')\n    parser.add_argument('command', choices=['backup', 'restore', 'list', 'cleanup', 'verify'])\n    parser.add_argument('--name', help='Backup name')\n    parser.add_argument('--file', help='Backup file path')\n    parser.add_argument('--type', choices=['full', 'database', 'data'], default='full')\n    \n    args = parser.parse_args()\n    \n    backup_system = PharmaBackupRecovery()\n    \n    try:\n        if args.command == 'backup':\n            if args.type == 'full':\n                result = await backup_system.create_full_backup(args.name)\n            elif args.type == 'database':\n                result = await backup_system.backup_database(args.name or 'manual_db')\n            elif args.type == 'data':\n                result = await backup_system.backup_application_data(args.name or 'manual_data')\n            \n            print(f\"Backup completed: {result['backup_name']}\")\n            \n        elif args.command == 'restore':\n            if not args.file:\n                print(\"Error: --file parameter required for restore\")\n                sys.exit(1)\n            \n            backup_file = Path(args.file)\n            if args.type == 'database':\n                success = await backup_system.restore_database(backup_file)\n            elif args.type == 'data':\n                success = await backup_system.restore_application_data(backup_file)\n            \n            print(f\"Restore {'successful' if success else 'failed'}\")\n            \n        elif args.command == 'list':\n            backups = await backup_system.list_backups()\n            print(f\"Found {len(backups)} backups:\")\n            for backup in backups:\n                print(f\"  {backup['backup_name']} - {backup['timestamp']} - {backup['backup_type']}\")\n        \n        elif args.command == 'cleanup':\n            removed = await backup_system.cleanup_old_backups()\n            print(f\"Removed {removed} old backups\")\n        \n        elif args.command == 'verify':\n            if not args.file:\n                print(\"Error: --file parameter required for verify\")\n                sys.exit(1)\n            \n            backup_file = Path(args.file)\n            valid = await backup_system.verify_backup_integrity(backup_file)\n            print(f\"Backup integrity: {'VALID' if valid else 'INVALID'}\")\n    \n    except Exception as e:\n        logger.error(f\"Operation failed: {str(e)}\")\n        sys.exit(1)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())"